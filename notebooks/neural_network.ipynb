{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94a24d22",
   "metadata": {},
   "source": [
    "## ニューラルネットワークモデル比較\n",
    "## (sklearn vs pytorch vs 手動)\n",
    "---\n",
    "\n",
    "### モデル構造\n",
    "\n",
    "- 入力層：特徴量数に依存（今回のデータセットでは8次元）  \n",
    "- 隠れ層：16ユニット、活性化関数はシグモイド関数  \n",
    "- 出力層：1ユニット、シグモイド活性化により確率を出力   \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "入力 X に対し、隠れ層・出力層の出力は以下のように計算：\n",
    "\n",
    "$$\n",
    "Z^{(1)} = X W_1 + b_1\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{(1)} = \\sigma(Z^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(2)} = A^{(1)} W_2 + b_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{(2)} = \\sigma(Z^{(2)})\n",
    "$$\n",
    " は入力特徴量の次元数\n",
    "\n",
    "ニューロンの出力はシグモイド関数で計算：\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 損失関数\n",
    "\n",
    "出力 $A^{(2)}$ と正解ラベル $y$ ,サンプル数$m$を用いて損失は\n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log A_i^{(2)} + (1 - y_i) \\log (1 - A_i^{(2)}) \\right]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### パラメータ更新（勾配降下法）\n",
    "\n",
    "学習率 $\\alpha$ を用いてパラメータを更新：\n",
    "\n",
    "$$\n",
    "W_1 \\leftarrow W_1 - \\alpha \\nabla_{W_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_1 \\leftarrow b_1 - \\alpha \\nabla_{b_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_2 \\leftarrow W_2 - \\alpha \\nabla_{W_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_2 \\leftarrow b_2 - \\alpha \\nabla_{b_2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9df5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b9ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n",
      "None\n",
      "(614,)\n",
      "Epoch 0: Loss = 0.9182\n",
      "Epoch 100: Loss = 0.5544\n",
      "Epoch 200: Loss = 0.4977\n",
      "Epoch 300: Loss = 0.4780\n",
      "Epoch 400: Loss = 0.4689\n",
      "Epoch 500: Loss = 0.4638\n",
      "Epoch 600: Loss = 0.4604\n",
      "Epoch 700: Loss = 0.4579\n",
      "Epoch 800: Loss = 0.4559\n",
      "Epoch 900: Loss = 0.4541\n",
      "time: 1.1240 秒\n",
      "Accuracy: 0.7403\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n#ミニバッチ\\nbatch_size = 32\\nfor epoch in range(epochs):\\n    for i in range(0, m, batch_size):\\n        X_batch = X_train_norm[i:i+batch_size]\\n        y_batch = y_train[i:i+batch_size]\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/diabetes.csv\")\n",
    "print(df.info())\n",
    "X = df.drop(\"Outcome\", axis=1).values\n",
    "y = df[\"Outcome\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(y_train.shape)\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "\n",
    "#手動ニューラルネットワーク（1層）実装\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "# 入力 → 中間層(16) → 出力(1)\n",
    "m, input_dim = X_train_norm.shape\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "alpha = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "#標準正規分布（平均0, 標準偏差1）で生成\n",
    "W1 = np.random.randn(input_dim, hidden_dim)\n",
    "b1 = np.zeros(hidden_dim)\n",
    "W2 = np.random.randn(hidden_dim, output_dim)\n",
    "b2 = np.zeros(output_dim)\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    # 計算\n",
    "    Z1 = np.dot(X_train_norm, W1) + b1 #(m,hidden_dim)\n",
    "    A1 = sigmoid(Z1) #(m,hidden_dim)\n",
    "    Z2 = np.dot(A1, W2) + b2 #(m,output_dim)\n",
    "    A2 = sigmoid(Z2)#(m,output_dim)\n",
    "    \n",
    "    # 損失（あえてaxis指定せずにスカラー化する）\n",
    "    epsilon = 1e-7\n",
    "    loss = -np.mean(y_train * np.log(A2 + epsilon) + (1 - y_train) * np.log(1 - A2 + epsilon))\n",
    "    \n",
    "    # 誤差逆伝搬法\n",
    "    #L/Z2, L/W2 = L/Z2 * Z2/W2, L/A1 = L/Z2 * Z2/A1, L/Z1 = L/A1 * A1/Z1\n",
    "    \n",
    "    dZ2 = A2 - y_train #(m,output_dim)np.array([[0.1], [-0.2], [0.3], ×m])\n",
    "    dW2 = np.dot(A1.T, dZ2) / m #(hidden_dim, output_dim) = (hidden_dim, m) ,dz([[0.1], [-0.2], [0.3], ...]) \n",
    "    db2 = np.mean(dZ2, axis=0) #(outout_dim,)\n",
    "    dA1 = np.dot(dZ2, W2.T) #(m, hidden_dim)\n",
    "    dZ1 = dA1 * sigmoid_derivative(Z1) #(m, hidden_dim)\n",
    "    dW1 = np.dot(X_train_norm.T, dZ1) / m #(input_dim, hidden_dim) W ij一個ずつの勾配を、ミニバッチ内のサンプル平均で計算\n",
    "    db1 = np.mean(dZ1, axis=0) #(hidden_dim,)\n",
    "    \n",
    "    # バッチ勾配降下法\n",
    "    W1 -= alpha * dW1\n",
    "    b1 -= alpha * db1\n",
    "    W2 -= alpha * dW2\n",
    "    b2 -= alpha * db2\n",
    "\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"time: {end_time - start_time:.4f} 秒\")\n",
    "\n",
    "# 評価\n",
    "Z1_test = np.dot(X_test_norm, W1) + b1\n",
    "A1_test = sigmoid(Z1_test)\n",
    "Z2_test = np.dot(A1_test, W2) + b2\n",
    "A2_test = sigmoid(Z2_test).flatten()\n",
    "y_pred_manual = (A2_test >= 0.5).astype(int) #np.where(A2_test >= 0.5, 1, 0)\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(f\"Accuracy: {acc_manual:.4f}\")\n",
    "\n",
    "\"\"\" \n",
    "#ミニバッチ\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    for i in range(0, m, batch_size):\n",
    "        X_batch = X_train_norm[i:i+batch_size]\n",
    "        y_batch = y_train[i:i+batch_size]\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021346b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyTorch NN] Epoch 0: Loss = 0.7865\n",
      "[PyTorch NN] Epoch 100: Loss = 0.5866\n",
      "[PyTorch NN] Epoch 200: Loss = 0.5327\n",
      "[PyTorch NN] Epoch 300: Loss = 0.4975\n",
      "[PyTorch NN] Epoch 400: Loss = 0.4791\n",
      "[PyTorch NN] Epoch 500: Loss = 0.4698\n",
      "[PyTorch NN] Epoch 600: Loss = 0.4650\n",
      "[PyTorch NN] Epoch 700: Loss = 0.4623\n",
      "[PyTorch NN] Epoch 800: Loss = 0.4607\n",
      "[PyTorch NN] Epoch 900: Loss = 0.4596\n",
      "[PyTorch NN] 実行時間: 0.8992 秒\n",
      "Accuracy (PyTorch NN): 0.7532\n"
     ]
    }
   ],
   "source": [
    "# PyTorchによる実装\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__() #親クラス（nn.Module）の初期化処理を呼び出す \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):#入力を受け取って出力を返す処理（順伝播）を定義\n",
    "        return self.net(x)\n",
    "\n",
    "# 入力次元\n",
    "input_dim = X_train_norm.shape[1]\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = SimpleNN(input_dim) #モデル構造をメモリ上に作る（クラス → インスタンス）\n",
    "criterion = nn.BCELoss() # クライテリオン=loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# テンソル(多次元配列)変換\n",
    "X_train_tensor = torch.tensor(X_train_norm, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "# 学習\n",
    "start_time = time.time()\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(X_train_tensor) #= model.forward(X_train_tensor)  \n",
    "    loss = criterion(y_pred, y_train_tensor)\n",
    "    optimizer.zero_grad() #前のループで計算した勾配リセット\n",
    "    loss.backward() #損失に対して逆伝播を実行し、各パラメータの勾配を計算\n",
    "    optimizer.step() #計算した勾配を使って、モデルのパラメータを更新\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"[PyTorch NN] Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "end_time = time.time()\n",
    "print(f\"[PyTorch NN] 実行時間: {end_time - start_time:.4f} 秒\")\n",
    "\n",
    "# 評価\n",
    "X_test_tensor = torch.tensor(X_test_norm, dtype=torch.float32)\n",
    "y_pred_torch = model(X_test_tensor).detach().numpy().flatten() #detach() で計算グラフから切り離す\n",
    "y_pred_torch_bin = (y_pred_torch >= 0.5).astype(int)\n",
    "acc_torch = accuracy_score(y_test, y_pred_torch_bin)\n",
    "print(f\"Accuracy (PyTorch NN): {acc_torch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64b9dce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bsksh\\Documents\\programm\\github\\machine_learning\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1102: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67893666\n",
      "Iteration 2, loss = 0.67330685\n",
      "Iteration 3, loss = 0.66765829\n",
      "Iteration 4, loss = 0.66298742\n",
      "Iteration 5, loss = 0.65940345\n",
      "Iteration 6, loss = 0.65646312\n",
      "Iteration 7, loss = 0.65358626\n",
      "Iteration 8, loss = 0.65034892\n",
      "Iteration 9, loss = 0.64659622\n",
      "Iteration 10, loss = 0.64241006\n",
      "Iteration 11, loss = 0.63800049\n",
      "Iteration 12, loss = 0.63358660\n",
      "Iteration 13, loss = 0.62931285\n",
      "Iteration 14, loss = 0.62522003\n",
      "Iteration 15, loss = 0.62126439\n",
      "Iteration 16, loss = 0.61736233\n",
      "Iteration 17, loss = 0.61343454\n",
      "Iteration 18, loss = 0.60943248\n",
      "Iteration 19, loss = 0.60534271\n",
      "Iteration 20, loss = 0.60117583\n",
      "Iteration 21, loss = 0.59695032\n",
      "Iteration 22, loss = 0.59268006\n",
      "Iteration 23, loss = 0.58836930\n",
      "Iteration 24, loss = 0.58401442\n",
      "Iteration 25, loss = 0.57960922\n",
      "Iteration 26, loss = 0.57515047\n",
      "Iteration 27, loss = 0.57064137\n",
      "Iteration 28, loss = 0.56609229\n",
      "Iteration 29, loss = 0.56151951\n",
      "Iteration 30, loss = 0.55694289\n",
      "Iteration 31, loss = 0.55238369\n",
      "Iteration 32, loss = 0.54786297\n",
      "Iteration 33, loss = 0.54340086\n",
      "Iteration 34, loss = 0.53901637\n",
      "Iteration 35, loss = 0.53472744\n",
      "Iteration 36, loss = 0.53055095\n",
      "Iteration 37, loss = 0.52650250\n",
      "Iteration 38, loss = 0.52259607\n",
      "Iteration 39, loss = 0.51884359\n",
      "Iteration 40, loss = 0.51525453\n",
      "Iteration 41, loss = 0.51183570\n",
      "Iteration 42, loss = 0.50859121\n",
      "Iteration 43, loss = 0.50552252\n",
      "Iteration 44, loss = 0.50262873\n",
      "Iteration 45, loss = 0.49990685\n",
      "Iteration 46, loss = 0.49735213\n",
      "Iteration 47, loss = 0.49495837\n",
      "Iteration 48, loss = 0.49271830\n",
      "Iteration 49, loss = 0.49062383\n",
      "Iteration 50, loss = 0.48866635\n",
      "Iteration 51, loss = 0.48683700\n",
      "Iteration 52, loss = 0.48512686\n",
      "Iteration 53, loss = 0.48352720\n",
      "Iteration 54, loss = 0.48202961\n",
      "Iteration 55, loss = 0.48062613\n",
      "Iteration 56, loss = 0.47930931\n",
      "Iteration 57, loss = 0.47807232\n",
      "Iteration 58, loss = 0.47690891\n",
      "Iteration 59, loss = 0.47581346\n",
      "Iteration 60, loss = 0.47478092\n",
      "Iteration 61, loss = 0.47380679\n",
      "Iteration 62, loss = 0.47288705\n",
      "Iteration 63, loss = 0.47201818\n",
      "Iteration 64, loss = 0.47119702\n",
      "Iteration 65, loss = 0.47042076\n",
      "Iteration 66, loss = 0.46968691\n",
      "Iteration 67, loss = 0.46899319\n",
      "Iteration 68, loss = 0.46833755\n",
      "Iteration 69, loss = 0.46771809\n",
      "Iteration 70, loss = 0.46713303\n",
      "Iteration 71, loss = 0.46658070\n",
      "Iteration 72, loss = 0.46605950\n",
      "Iteration 73, loss = 0.46556787\n",
      "Iteration 74, loss = 0.46510431\n",
      "Iteration 75, loss = 0.46466734\n",
      "Iteration 76, loss = 0.46425550\n",
      "Iteration 77, loss = 0.46386736\n",
      "Iteration 78, loss = 0.46350151\n",
      "Iteration 79, loss = 0.46315656\n",
      "Iteration 80, loss = 0.46283114\n",
      "Iteration 81, loss = 0.46252390\n",
      "Iteration 82, loss = 0.46223356\n",
      "Iteration 83, loss = 0.46195885\n",
      "Iteration 84, loss = 0.46169853\n",
      "Iteration 85, loss = 0.46145146\n",
      "Iteration 86, loss = 0.46121650\n",
      "Iteration 87, loss = 0.46099259\n",
      "Iteration 88, loss = 0.46077874\n",
      "Iteration 89, loss = 0.46057399\n",
      "Iteration 90, loss = 0.46037747\n",
      "Iteration 91, loss = 0.46018836\n",
      "Iteration 92, loss = 0.46000591\n",
      "Iteration 93, loss = 0.45982941\n",
      "Iteration 94, loss = 0.45965825\n",
      "Iteration 95, loss = 0.45949183\n",
      "Iteration 96, loss = 0.45932963\n",
      "Iteration 97, loss = 0.45917119\n",
      "Iteration 98, loss = 0.45901607\n",
      "Iteration 99, loss = 0.45886390\n",
      "Iteration 100, loss = 0.45871435\n",
      "Iteration 101, loss = 0.45856710\n",
      "Iteration 102, loss = 0.45842191\n",
      "Iteration 103, loss = 0.45827853\n",
      "Iteration 104, loss = 0.45813676\n",
      "Iteration 105, loss = 0.45799642\n",
      "Iteration 106, loss = 0.45785736\n",
      "Iteration 107, loss = 0.45771944\n",
      "Iteration 108, loss = 0.45758254\n",
      "Iteration 109, loss = 0.45744656\n",
      "Iteration 110, loss = 0.45731141\n",
      "Iteration 111, loss = 0.45717701\n",
      "Iteration 112, loss = 0.45704331\n",
      "Iteration 113, loss = 0.45691023\n",
      "Iteration 114, loss = 0.45677774\n",
      "Iteration 115, loss = 0.45664579\n",
      "Iteration 116, loss = 0.45651435\n",
      "Iteration 117, loss = 0.45638338\n",
      "Iteration 118, loss = 0.45625286\n",
      "Iteration 119, loss = 0.45612277\n",
      "Iteration 120, loss = 0.45599310\n",
      "Iteration 121, loss = 0.45586382\n",
      "Iteration 122, loss = 0.45573492\n",
      "Iteration 123, loss = 0.45560640\n",
      "Iteration 124, loss = 0.45547825\n",
      "Iteration 125, loss = 0.45535045\n",
      "Iteration 126, loss = 0.45522302\n",
      "Iteration 127, loss = 0.45509594\n",
      "Iteration 128, loss = 0.45496921\n",
      "Iteration 129, loss = 0.45484283\n",
      "Iteration 130, loss = 0.45471681\n",
      "Iteration 131, loss = 0.45459114\n",
      "Iteration 132, loss = 0.45446583\n",
      "Iteration 133, loss = 0.45434088\n",
      "Iteration 134, loss = 0.45421628\n",
      "Iteration 135, loss = 0.45409206\n",
      "Iteration 136, loss = 0.45396819\n",
      "Iteration 137, loss = 0.45384470\n",
      "Iteration 138, loss = 0.45372159\n",
      "Iteration 139, loss = 0.45359885\n",
      "Iteration 140, loss = 0.45347650\n",
      "Iteration 141, loss = 0.45335454\n",
      "Iteration 142, loss = 0.45323296\n",
      "Iteration 143, loss = 0.45311178\n",
      "Iteration 144, loss = 0.45299100\n",
      "Iteration 145, loss = 0.45287062\n",
      "Iteration 146, loss = 0.45275065\n",
      "Iteration 147, loss = 0.45263109\n",
      "Iteration 148, loss = 0.45251195\n",
      "Iteration 149, loss = 0.45239322\n",
      "Iteration 150, loss = 0.45227492\n",
      "Iteration 151, loss = 0.45215703\n",
      "Iteration 152, loss = 0.45203958\n",
      "Iteration 153, loss = 0.45192256\n",
      "Iteration 154, loss = 0.45180598\n",
      "Iteration 155, loss = 0.45168983\n",
      "Iteration 156, loss = 0.45157412\n",
      "Iteration 157, loss = 0.45145886\n",
      "Iteration 158, loss = 0.45134405\n",
      "Iteration 159, loss = 0.45122968\n",
      "Iteration 160, loss = 0.45111577\n",
      "Iteration 161, loss = 0.45100232\n",
      "Iteration 162, loss = 0.45088932\n",
      "Iteration 163, loss = 0.45077679\n",
      "Iteration 164, loss = 0.45066472\n",
      "Iteration 165, loss = 0.45055312\n",
      "Iteration 166, loss = 0.45044199\n",
      "Iteration 167, loss = 0.45033134\n",
      "Iteration 168, loss = 0.45022116\n",
      "Iteration 169, loss = 0.45011145\n",
      "Iteration 170, loss = 0.45000223\n",
      "Iteration 171, loss = 0.44989349\n",
      "Iteration 172, loss = 0.44978524\n",
      "Iteration 173, loss = 0.44967748\n",
      "Iteration 174, loss = 0.44957021\n",
      "Iteration 175, loss = 0.44946343\n",
      "Iteration 176, loss = 0.44935715\n",
      "Iteration 177, loss = 0.44925136\n",
      "Iteration 178, loss = 0.44914608\n",
      "Iteration 179, loss = 0.44904130\n",
      "Iteration 180, loss = 0.44893702\n",
      "Iteration 181, loss = 0.44883325\n",
      "Iteration 182, loss = 0.44872999\n",
      "Iteration 183, loss = 0.44862724\n",
      "Iteration 184, loss = 0.44852500\n",
      "Iteration 185, loss = 0.44842328\n",
      "Iteration 186, loss = 0.44832206\n",
      "Iteration 187, loss = 0.44822137\n",
      "Iteration 188, loss = 0.44812119\n",
      "Iteration 189, loss = 0.44802154\n",
      "Iteration 190, loss = 0.44792240\n",
      "Iteration 191, loss = 0.44782378\n",
      "Iteration 192, loss = 0.44772569\n",
      "Iteration 193, loss = 0.44762812\n",
      "Iteration 194, loss = 0.44753107\n",
      "Iteration 195, loss = 0.44743455\n",
      "Iteration 196, loss = 0.44733855\n",
      "Iteration 197, loss = 0.44724307\n",
      "Iteration 198, loss = 0.44714812\n",
      "Iteration 199, loss = 0.44705369\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Test accuracy: 0.7468\n",
      "Training time: 0.3987 seconds\n"
     ]
    }
   ],
   "source": [
    "# sklearnによる実装\n",
    "# モデル定義\n",
    "model = MLPClassifier(hidden_layer_sizes=(16,),\n",
    "                      activation='logistic',  # sigmoid活性化\n",
    "                      solver='sgd',\n",
    "                      learning_rate_init=0.1,\n",
    "                      learning_rate = 'constant',\n",
    "                      max_iter=1000,\n",
    "                      random_state=42,\n",
    "                      batch_size=X_train_norm.shape[0], \n",
    "                      verbose=True)  # 学習ログを表示\n",
    "\n",
    "start_time = time.time()\n",
    "model.fit(X_train_norm, y_train)\n",
    "end_time = time.time()\n",
    "# 予測\n",
    "y_pred = model.predict(X_test_norm)\n",
    "#　精度\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "print(f\"Training time: {end_time - start_time:.4f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
